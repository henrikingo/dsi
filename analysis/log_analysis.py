"""
Functions for analyzing `mongod` log files for suspect messages. Useful for sanity checking that
everything ran smoothly during a test.
"""

from __future__ import print_function
import os
import os.path
import logging
import datetime
from dateutil import tz, parser as date_parser
import util

LOGGER = logging.getLogger(__name__)

def analyze_logs(reports_dir_path, perf_file_path=None):
    """
    Analyze all the "mongod.log" logs in the directory tree rooted at `reports_dir_path`, and return
    a list of test-result dictionaries ready to be placed in the report JSON generated by
    `post_run_check`/`perf_regression_check`. If you want to only analyze log messages generated during the time of an
    actual test run, and not test setup/transition, then set `perf_file_path` to the path of the performance results
    file (probably `perf.json`) generated by the test runner (benchrun or mission-control), which contains relevant
    timestamp data.
    """

    LOGGER.info("Analyzing logs")
    results = []
    num_failures = 0

    test_times = None if perf_file_path is None else _get_test_times(perf_file_path)
    bad_logs = _get_bad_log_lines(reports_dir_path, test_times)

    for log_num, (log_path, bad_lines) in enumerate(bad_logs):
        result = {
            "status": "fail" if bad_lines else "pass",
            "log_raw": _format_log_raw(log_path, bad_lines),
            "test_file": "mongod.log.{0}".format(log_num),
            "start": 0,
            "exit_code": 1 if bad_lines else 0
        }
        results.append(result)

        if bad_lines:
            num_failures += 1

    return results, num_failures

def _format_log_raw(path, bad_lines):
    """
    Return a nicely formatted `log_raw` message for a log file at path `path` with bad messages `bad_lines` (could be
    empty, indicating a passing test).
    """

    msg_path_header = "\nLog file: {0}\n".format(path)
    msg_body = "No bad messages found" if not bad_lines else \
        "Number of bad lines: {0}\nBad lines below: \n{1}\n{2}".format(path, len(bad_lines), "".join(bad_lines))
    return msg_path_header + msg_body

def _get_bad_log_lines(reports_dir_path, test_times=None):
    """
    Recursively search the directory `reports_dir_path` for files called "mongod.log" and identify
    bad messages in each. `test_times` is a list of `(start, end)` `datetime` tuples specifying the
    start and end times of the actual tests that ran, so that we can ignore log messages generated
    during a test setup/transition phase. Return a list of (path, bad_messages) tuples, where `path`
    is the path of the `mongod.log` and `bad_messages` is a list of the bad messages.
    """

    bad_messages_per_log = []
    for path in _get_log_file_paths(reports_dir_path):
        LOGGER.info("Analyzing log file: " + path)
        with open(path) as log_file:
            bad_messages = [
                line for line in log_file if line != "\n" and _is_log_line_bad(line, test_times)]
        bad_messages_per_log.append((path, bad_messages))

    return bad_messages_per_log

def _get_log_file_paths(dir_path):
    """
    Recursively search `dir_path` for files called "mongod.log" and return a list of their fully
    qualified paths.
    """

    log_filename = "mongod.log"
    log_paths = []
    for sub_dir_path, _, filenames in os.walk(dir_path):
        if log_filename in filenames:
            log_paths.append(os.path.join(sub_dir_path, log_filename))

    return log_paths

# TODO: break out the following into a config file
# https://jira.mongodb.org/browse/PERF-603

BAD_LOG_TYPES = ["F", "E"] # See https://docs.mongodb.com/manual/reference/log-messages/
BAD_MESSAGES = [msg.lower() for msg in [
    "starting an election", "election succeeded", "transition to primary"]]

def _is_log_line_bad(log_line, test_times=None):
    """
    Return whether or not `log_line`, a line from a log file, is suspect (see `BAD_LOG_TYPES` and
    `BAD_MESSAGES`). Only messages that were printed during the time a test was run (as specified in
    `test_times`) are considered, unless `test_times` is None.
    """

    log_line = log_line.strip()
    line_components = log_line.split(" ", 3)
    if len(line_components) != 4:
        LOGGER.warning("Couldn't parse log line: `%s`", log_line)
        return False

    timestamp, err_type_char, _, log_msg = line_components

    try:
        log_ts = date_parser.parse(timestamp)
    except ValueError as err:
        LOGGER.warning("Failed to parse timestamp from line `%s` with error `%s`", log_line, err)
        return False

    if test_times is not None and not any(start <= log_ts <= end for start, end in test_times):
        return False

    log_msg = log_msg.lower()
    return err_type_char in ["F", "E"] or any(bad_msg in log_msg for bad_msg in BAD_MESSAGES)

def _get_test_times(perf_file_path):
    """
    Read the performance report file at `perf_file_path` (usually called "perf.json") and return a
    `(start, end)` tuple of its "start" and "end" timestamps, represented as UTC `datetime`s.
    """

    LOGGER.info("Getting test times from `%s`", perf_file_path)
    try:
        perf_json = util.get_json(perf_file_path)

    except IOError:
        LOGGER.error("Failed to read file `%s`", perf_file_path)
        return None

    return [(_num_or_str_to_date(perf_json["start"]), _num_or_str_to_date(perf_json["end"]))]

def _num_or_str_to_date(ts_or_date_str):
    """
    Convert `ts_or_date_str`, which is either a seconds-since-epoch `float` or a formatted date
    string, to a `datetime` object.
    """

    conv_func = date_parser.parse if isinstance(ts_or_date_str, basestring) else \
        _unix_ts_to_utc_datetime
    return conv_func(ts_or_date_str)

def _unix_ts_to_utc_datetime(unix_ts):
    """
    Convert `unix_ts` (a seconds-since-epoch `float`) to a UTC `datetime` object with
    `tzinfo=tzutc()`.
    """

    datetime_ts = datetime.datetime.utcfromtimestamp(unix_ts)
    return datetime_ts.replace(tzinfo=tz.tzutc())
