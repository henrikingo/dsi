# Defaults for all config values used by Distributed Performance 2.0 modules.

bootstrap:
  infrastructure_provisioning: single
  platform: linux
  mongodb_setup: standalone
  workload_setup: common
  analysis: common
  storageEngine: wiredTiger
  test_control: core
  production: false
  # Note: It's also allowed to upload your own binary. You can unset this by setting to the empty
  # string "" in bootstrap.yml
  # This URL is r4.0.2. We use the sys-perf build instead of official release to include the jstests/ directory that we copy in our compile task.
  mongodb_binary_archive: https://s3.amazonaws.com/mciuploads/dsi/sys_perf_4.0_fc1573ba18aee42f97a3bb13b67af7d837826b47/fc1573ba18aee42f97a3bb13b67af7d837826b47/linux/mongodb-sys_perf_4.0_fc1573ba18aee42f97a3bb13b67af7d837826b47.tar.gz
  workloads_dir: ./workloads
  ycsb_dir: ./YCSB
  linkbench_dir: ./linkbench
  tpcc_dir: ./tpcc
  genny_dir: ./genny
  authentication: disabled

infrastructure_provisioning:
  tfvars:
    cluster_name: default_cluster_name
    mongod_instance_count: 1
    workload_instance_count: 1

    mongod_instance_type: c3.8xlarge
    workload_instance_type: c3.8xlarge

    region: us-west-2
    availability_zone: us-west-2a

    ssh_user: ec2-user
    ssh_key_name: serverteam-perf-ssh-key
    ssh_key_file: aws_ssh_key.pem

    tags:
      expire-on-delta: 2     # adjust expire_on to today + expire-on-delta days
      owner: serverteam-perf@10gen.com
      Project: sys-perf
  numactl_prefix: numactl --interleave=all --cpunodebind=1
  evergreen:
    reuse_cluster: ${bootstrap.production}
    data_dir: /data/infrastructure_provisioning
  terraform:
    required_version: Terraform v0.10.4
    aws_required_version: 1.6.0

mongodb_setup:
  mongo_dir: mongodb
  journal_dir: /media/ephemeral1/journal
  clean_db_dir: true
  mongodb_binary_archive: ${bootstrap.mongodb_binary_archive}
  mongod_config_file:  # Note these defaults can be overridden by user, but not unset.
    net: ${mongodb_setup.authentication.${bootstrap.authentication}.net}
    processManagement:
      fork: true
    replication:
      oplogSizeMB: 153600   # 150GB oplog
    setParameter:
      enableTestCommands: 1
      # Exhaust cursor feature support is available only since MongoDB 4.1.4.
      # To enable/disable exhaust cursor support for initial sync collection cloner.
      # Default is true (i.e. exhaust cursor support is enabled) since MongoDB 4.1.4.
      collectionClonerUsesExhaust: true
    storage:
      dbPath: data/dbs
      engine: wiredTiger
    systemLog:
      destination: file
      path: data/logs/mongod.log

  mongos_config_file:
    net: ${mongodb_setup.authentication.${bootstrap.authentication}.net}
    processManagement:
      fork: true
    setParameter:
      enableTestCommands: 1
    systemLog:
      destination: file
      path: data/logs/mongos.log

  rs_conf:
    settings:
      chainingAllowed: false

  configsvr_config_file:
    net: ${mongodb_setup.authentication.${bootstrap.authentication}.net}
    processManagement:
      fork: true
    setParameter:
      enableTestCommands: 1
    storage:
      dbPath: data/dbs
      engine: wiredTiger
    systemLog:
      destination: file
      path: data/logs/mongod.log

  # these options are passed to the mongo shutdownServer command
  shutdown_options:
    force: true
    timeoutSecs: 30

  # these timeouts specify the max amount of milliseconds that the mongo shutdownServer command and
  # pkill mongo commands can run for.
  timeouts:
    shutdown_ms: 540000
    sigterm_ms: 60000

  meta:
    # The list of hosts that can be used in a mongodb connection string
    hosts: ${mongodb_setup.topology.0.private_ip}:27017
    is_sharded: false
    is_replset: false
    secondary: None

    # Meta-data used to determine scale_factor for a cluster
    storageEngine: ${mongodb_setup.mongod_config_file.storage.engine}
    # Number of primaries in the cluster. For non-sharded this
    # should be 1. For sharded will be the number of shards.
    primaries: 1
    secondaries: 0  # Secondaries per shard

  post_test:
    - on_all_servers:
        retrieve_files:
          - source: data/logs/
            target: ./
    - on_mongod:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data
    - on_configsvr:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data
  post_task:
    - on_all_servers:
        exec: |
          set -v
          rm -rf ./mdiag
          mkdir -p ./mdiag
          wget -q -O-  https://raw.githubusercontent.com/mongodb/support-tools/master/mdiag/mdiag.sh > ./mdiag/mdiag.sh
          # The "sys-perf" paramter to mdiag.sh is the 'reference'. As this is a support
          # tool, the reference would normally be a support case identifier.
          # For this example, it can be anything so that we don't
          # get a warning.
          # The output is written to "$TMPDIR/mdiag-$(hostname).json".
          # This json file is machine readable, rendering the
          # output to a more hunan friendly format is not done here.
          sudo TMPDIR=./mdiag bash ./mdiag/mdiag.sh sys-perf
    - on_all_servers:
        retrieve_files:
          - source: mdiag/
            target: ./

  # Enabled and disabled authentication and SSL settings.
  authentication:
    enabled:
      net:
        port: 27017
        bindIp: 0.0.0.0
        # Due to the state of regressions when SSL is enabled, we are turning off SSL until after
        # 4.0 is released.
        # ssl:
        #   mode: preferSSL
        #   PEMKeyFile: ${mongodb_setup.mongo_dir}/member.pem
        #   PEMKeyPassword: server-perf
        #   CAFile: ${mongodb_setup.mongo_dir}/root.crt
      username: username
      password: password
      # Temporarily disable SSL.
      mongodb_url: mongodb://${mongodb_setup.authentication.enabled.username}:${mongodb_setup.authentication.enabled.password}@${mongodb_setup.meta.hosts}/admin?ssl=false
      # Get necessary SSL keys.
      pre_cluster_start:
        - on_all_hosts:
            upload_repo_files:
              - source: configurations/mongodb_setup/ssl/member.pem
                target: ${mongodb_setup.mongo_dir}/member.pem
              - source: configurations/mongodb_setup/ssl/root.crt
                target: ${mongodb_setup.mongo_dir}/root.crt
        - on_workload_client:
            upload_repo_files:
              - source: configurations/mongodb_setup/ssl/client.crt
                target: ${mongodb_setup.mongo_dir}/client.crt
              - source: configurations/mongodb_setup/ssl/client.key
                target: ${mongodb_setup.mongo_dir}/client.key
              - source: configurations/mongodb_setup/ssl/keystore.jks
                target: ${mongodb_setup.mongo_dir}/keystore.jks
              - source: configurations/mongodb_setup/ssl/root.crt
                target: ${mongodb_setup.mongo_dir}/root.crt
        - on_workload_client:
            exec: |
              export JAVA_HOME="/usr/java/jdk1.8.0_162"
              sudo $JAVA_HOME/bin/keytool -importcert -trustcacerts -file ${mongodb_setup.mongo_dir}/root.crt -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit -noprompt || true
    disabled:
      net:
        port: 27017
        bindIp: 0.0.0.0
      mongodb_url: mongodb://${mongodb_setup.meta.hosts}/admin?ssl=false
      # This is just a placeholder for pre_cluster_start when we are not using SSL.
      pre_cluster_start:
        - on_workload_client:
            exec: |
              ls

  pre_cluster_start: ${mongodb_setup.authentication.${bootstrap.authentication}.pre_cluster_start}

  upon_error:
    - on_all_servers:
        exec: |
          (df; du  --max-depth 1 ./data) | sed "s/^/$(hostname): /"
        retrieve_files:
          - source: data/logs/
            target: ./
    - on_mongod:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data
    - on_configsvr:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data

# While the user is now available as an Evergreen expansion, older builds will fail when it is
# missing from system_perf.yml. Thus, we must store it here as a default value.
runtime_secret:
  dsi_analysis_atlas_user: dsi_analysis

test_control:
  task_name: default_task_name
  timeouts:
    no_output_ms: 5400000  # 90 minutes
  jstests_dir: ./jstests/hooks
  between_tests:
    - restart_mongodb:
        clean_logs: true
        clean_db_dir: true

  output_file:
    mongoshell: test_output.log
    ycsb: test_output.log
    fio: fio.json
    iperf: iperf.json
    tpcc: results.log
    genny: genny-perf.json

  # As I'm writing this, the basename IS the name. PERF-1096 will add a unique component.
  reports_dir_basename: reports
  perf_json:
    path: perf.json

  common_fio_config: |
    [global]
    directory=./data/fio
    filename_format=fiofile.$jobnum.$filenum
    size=1G
    runtime=120
    time_based
    group_reporting
    ioengine=libaio
    direct=1

    [setupfiles]
    stonewall
    filesize=1G
    nrfiles=16
    filename_format=fiofile.$filenum.0
    rw=randrw
    numjobs=1
    runtime=1

    [latency_test]
    stonewall
    description=This is for random read and write latency. 1 at a time
    rw=randrw
    numjobs=1
    bs=1
    ioengine=sync
    direct=0
    filesize=1G
    nrfiles=16
    filename_format=fiofile.$filenum.0
    write_bw_log=fiolatency
    write_lat_log=fiolatency
    write_iops_log=fiolatency

    [iops_test]
    stonewall
    description=How many iops can I sustain in parallel
    rw=randrw
    numjobs=32
    bs=4k
    iodepth=32
    write_bw_log=fioiops
    write_lat_log=fioiops
    write_iops_log=fioiops

    [streaming_bandwidth_test]
    stonewall
    description=Measure streaming bandwidth
    rw=rw
    numjobs=16
    bs=16k
    iodepth=32
    write_bw_log=fiostreaming
    write_lat_log=fiostreaming
    write_iops_log=fiostreaming

  common_fio_net_config: |
    [global]
    ioengine=net
    port=27019
    protocol=tcp
    bs=4k
    size=1g
    runtime=20
    time_based
    pingpong=1

    [sender]
    hostname=${mongodb_setup.meta.hostname}
    rw=write

  common_fio_net_config_listener: |
    [global]
    ioengine=net
    port=27019
    protocol=tcp
    bs=4k
    size=1g
    runtime=40
    time_based
    pingpong=1

    [receiver]
    listen
    rw=read

  ycsb_sharding_script: |
    if ("${mongodb_setup.meta.is_sharded}" == "True") {
      (function () {
        var err;
        for (var i = 0; i < 20; i++) {
          try {
            assert.commandWorked(sh.enableSharding("ycsb"));
            assert.commandWorked(
              sh.shardCollection("ycsb.usertable", {_id: "hashed"}));
            db.printShardingStatus();
            return;
          } catch (e) {
            err = e;
            sleep(1000);
          }
        }
        throw err;
      })()
    } else {
      print ("Non-sharded cluster");
    }

  bestbuy_sharding_script: |
    if ("${mongodb_setup.meta.is_sharded}" == "True") {

      // The following variables that are specific to the workload.
      // NOTE: the splits must be in order.
      var dbName = 'bestbuy';
      var sourceNs = 'bestbuy.products';

      // In addition to sharding the main namespace of the workload, we also set up some collections
      // which are initially empty but meant to be targeted by an $out stage during the test.
      var targetNsIdenticalDistributionSameDB = 'bestbuy.target_identical_distribution';
      var targetNsRangeIdSameDB = 'bestbuy.target_range_id';
      var targetDBName = 'target';
      var targetNsIdenticalDistributionOtherDB = 'target.identical_distribution';
      var targetNsIdHashedOtherDB = 'target.hashed_id';

      var keyPattern = {type: 1, productId: 1, _id: 1};
      var typeAndProductSplits = [
        {type: MinKey,     productId: MinKey, _id: 0},
        {type: 'Game',     productId: NumberLong('1218086489034'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1118840368488'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218105505555'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218379812886'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218475610897'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218618813745'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218728444211'), _id: 0}, // 8,9
        {type: 'HardGood', productId: NumberLong('1219080834391'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1219736622160'), _id: 0}, // 7
        {type: 'Movie',    productId: NumberLong('28132'), _id: 0},         // 6
        {type: 'Movie',    productId: NumberLong('44465'), _id: 0},
        {type: 'Movie',    productId: NumberLong('59159'), _id: 0},         // 5
        {type: 'Movie',    productId: NumberLong('1344253'), _id: 0},
        {type: 'Movie',    productId: NumberLong('1459047'), _id: 0},       // 8,9
        {type: 'Movie',    productId: NumberLong('1627489'), _id: 0},       // 4
        {type: 'Movie',    productId: NumberLong('1921771'), _id: 0},
        {type: 'Movie',    productId: NumberLong('2180510'), _id: 0},
        {type: 'Movie',    productId: NumberLong('2702290'), _id: 0},       // 7
        {type: 'Movie',    productId: NumberLong('3313579'), _id: 0},
        {type: 'Music',    productId: NumberLong('73927'), _id: 0},         // 3,6
        {type: 'Music',    productId: NumberLong('97408'), _id: 0},         // 8,9
        {type: 'Music',    productId: NumberLong('120846'), _id: 0},
        {type: 'Music',    productId: NumberLong('145563'), _id: 0},
        {type: 'Music',    productId: NumberLong('172542'), _id: 0},        // 5
        {type: 'Music',    productId: NumberLong('199025'), _id: 0},
        {type: 'Music',    productId: NumberLong('226846'), _id: 0},
        {type: 'Music',    productId: NumberLong('261778'), _id: 0},        // 7
        {type: 'Music',    productId: NumberLong('301467'), _id: 0},        // 8,9
        {type: 'Music',    productId: NumberLong('1337396'), _id: 0},
        {type: 'Music',    productId: NumberLong('1393391'), _id: 0},       // 4,6
        {type: 'Music',    productId: NumberLong('1443146'), _id: 0},
        {type: 'Music',    productId: NumberLong('1484664'), _id: 0},
        {type: 'Music',    productId: NumberLong('1531482'), _id: 0},
        {type: 'Music',    productId: NumberLong('1582679'), _id: 0},
        {type: 'Music',    productId: NumberLong('1624125'), _id: 0},       // 8,9
        {type: 'Music',    productId: NumberLong('1686112'), _id: 0},       // 5,7
        {type: 'Music',    productId: NumberLong('1778529'), _id: 0},
        {type: 'Music',    productId: NumberLong('1815087'), _id: 0},
        {type: 'Music',    productId: NumberLong('1858555'), _id: 0},
        {type: 'Music',    productId: NumberLong('1920647'), _id: 0},       // 3,6
        {type: 'Music',    productId: NumberLong('1992906'), _id: 0},
        {type: 'Music',    productId: NumberLong('2067260'), _id: 0},       // 8,9
        {type: 'Music',    productId: NumberLong('2149519'), _id: 0},
        {type: 'Music',    productId: NumberLong('2229905'), _id: 0},
        {type: 'Music',    productId: NumberLong('2305925'), _id: 0},       // 4
        {type: 'Music',    productId: NumberLong('2428992'), _id: 0},
        {type: 'Music',    productId: NumberLong('2558418'), _id: 0},
        {type: 'Music',    productId: NumberLong('2656772'), _id: 0},       // 5,7
        {type: 'Music',    productId: NumberLong('2709433'), _id: 0},       // 8,9
        {type: 'Music',    productId: NumberLong('2776923'), _id: 0},       // 6
        {type: 'Music',    productId: NumberLong('2820755'), _id: 0},
        {type: 'Music',    productId: NumberLong('2864319'), _id: 0},
        {type: 'Music',    productId: NumberLong('2900297'), _id: 0},
        {type: 'Music',    productId: NumberLong('2933428'), _id: 0},
        {type: 'Music',    productId: NumberLong('2971126'), _id: 0},
        {type: 'Music',    productId: NumberLong('3230451'), _id: 0},       // 7,8,9
        {type: 'Music',    productId: NumberLong('3310959'), _id: 0},
        {type: 'Music',    productId: NumberLong('3383162'), _id: 0},
        {type: 'Music',    productId: NumberLong('3514683'), _id: 0},
      ];

      // Get a list of the shard names.
      var config = db.getSiblingDB("config");
      var shards = config.shards.find({},{_id:1}).map(function(s){return s._id;});

      // Utility function to split a chunk in the middle.
      // See https://docs.mongodb.com/manual/reference/command/split/.
      // The split document must contain:
      //     ns {string} the collection names.
      //     middle {document} the split point.
      var splitter = function(split){
        return db.adminCommand({split: split.ns, middle: split.middle});
      };
      // Utility function to move a chunk.
      // See https://docs.mongodb.com/manual/reference/command/moveChunk/
      // The split document must contain:
      //     ns {string} the collection names
      //     middle {document} the split point
      //     shard {string} the destination shard
      var mover = function(split){
        return db.adminCommand({moveChunk: split.ns, find: split.middle, to: split.shard});
      };

      assert.commandWorked(sh.enableSharding(dbName));
      assert.commandWorked(sh.enableSharding(targetDBName));
      assert.commandWorked(sh.shardCollection(sourceNs, keyPattern));
      assert.commandWorked(sh.shardCollection(targetNsIdenticalDistributionSameDB, keyPattern, /*unique=*/true));
      assert.commandWorked(sh.shardCollection(targetNsIdenticalDistributionOtherDB, keyPattern, /*unique=*/true));

      // For a shard key {_id: "hashed"}, MongoDB will automatically split the collection into
      // two chunks per shard so no further work is necessary.
      assert.commandWorked(sh.shardCollection(targetNsIdHashedOtherDB, {_id: "hashed"}));

      var idAlphabetSplits = "abcdefghijklmnopqrstuvwxyz".split("").map(function(letter) {
          return {_id: letter};
      });
      assert.commandWorked(sh.shardCollection(targetNsRangeIdSameDB, {_id: 1}));

      var distributeColl = function (ns, splits) {

        // Given the split points 'splits' and 'nShards' shards, we try to partition the data such
        // that each shard has one chunk, but each chunk is of a similar size. Assuming there is a
        // similar amount of data between each split point, this means dividing the splits into
        // 'nShards' equal chunks.
        //
        // For example, if there are 31 entries in 'splits' and 3 shards, we take the 0th, 11th,
        // and 22nd entries, resulting in a filtered array of length 3.
        //
        // The map function creates a new document of the format required by the splitter and mover
        // functions. That is, it creates a document with:
        //    shard {string} the destination shard
        //    ns {string} the fully qualified collection namespace
        //    middle {document} the split point
        //
        // For a reasonable number of splits filter is ok. For a very large number
        // it will perform poorly. BUT, even in this case, it will probably be negligible.
        var distribution =  splits.filter(function(element, index){
          return index % Math.ceil(splits.length / shards.length) == 0;
        }).map(function(middle, i) {
          return {shard:shards[i % shards.length], middle:middle, ns: ns};
        });

        // Create the split points.
        // NOTE: to create N chunks, you only need N - 1 split points.
        distribution.slice(1).forEach(function(split) {
          assert.commandWorked(splitter(split));
        });

        // Move the chunks to specific shards.
        distribution.forEach(function(split){ assert.commandWorked(mover(split));});
      };

      // Disable balancing. If the split points are fair, then the cluster will still be in balance
      // at the end of the restore.
      sh.stopBalancer();

      distributeColl(sourceNs, typeAndProductSplits);
      distributeColl(targetNsIdenticalDistributionSameDB, typeAndProductSplits);
      distributeColl(targetNsIdenticalDistributionOtherDB, typeAndProductSplits);
      distributeColl(targetNsRangeIdSameDB, idAlphabetSplits);

      db.printShardingStatus();
    } else {
      print ("Non-sharded cluster");
    }

# add a delay before each test. Units are seconds but the value can be an int or float.
#  test_delay_seconds: 1
