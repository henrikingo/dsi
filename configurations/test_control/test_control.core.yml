task_name: core
run:
  - id: benchRun
    type: shell

    # This line to be shortened after the config file is copied from below into the production server.
    cmd: cd workloads && ${infrastructure_provisioning.numactl_prefix} ./run_workloads.py -c workloads.yml
    config_filename: workloads.yml  # The name used in previous row
    workload_config:
      ########################################################################
      # Test lists. Command line will indicate which list of tests to execute.
      # The 'default' list is the default.
      # When testing new test workloads, please put your tests in the
      # tests.test list, and remove the existing hello.js
      ########################################################################
      tests:
        default:
          - contended_update.js:
              thread_levels: [1, 32, 64]
          - map_reduce.js
          - insert_ttl.js:
              thread_levels: [1, 16, 32]
          - insert_vector.js:
              thread_levels: ${test_control.thread_levels.insert_vector.${mongodb_setup.meta.mongodb_setup}.${mongodb_setup.meta.storageEngine}}
          - word_count.js
          - crud.js:
              thread_levels: ${test_control.thread_levels.crud-jfalse.${mongodb_setup.meta.storageEngine}}
              j_value: false
          - crud.js:
              thread_levels: [1, 64, 128]
              j_value: true
          - index_build.js
          - mix.js:
              thread_levels: [4, 64, 128, 256, 512]
          - cpu_noise.js
        test:
          -  # Empty list. Put new workloads here for testing
        short:  # Just does Hello World
          - hello.js:
              parameter1: foo
              parameter2: true

      # These next five lines match existing workloads.yml.
      target: ${mongodb_setup.meta.hostname}
      port: ${mongodb_setup.meta.port}
      # Hard coding for now. These aren't working right now
      sharded: ${mongodb_setup.meta.is_sharded}
      replica: ${mongodb_setup.meta.is_replset}
    pre_test:
      - on_workload_client:
          upload_files:
            workloads.yml: workloads/workloads.yml
    post_test:
      - on_workload_client:
          retrieve_files:
            workloads/workload_timestamps.csv: ../workloads_timestamps.csv

  - id: fio
    type: shell
    cmd: '${infrastructure_provisioning.numactl_prefix} ./fio-test.sh ${mongodb_setup.meta.hostname}'
    config_filename: fio.ini
    workload_config: ${test_control.common_fio_config}
    pre_test:
      - on_workload_client:
          upload_files:
            fio.ini: fio.ini
    post_test:
      - on_workload_client:
          retrieve_files:
            fio.json: ../fio.json

  - id: iperf
    type: shell
    cmd: '${infrastructure_provisioning.numactl_prefix} ./iperf-test.sh ${mongodb_setup.meta.hostname}'
    post_test:
      - on_workload_client:
          retrieve_files:
            iperf.json: ../iperf.json

# This is just a lookup table. Each test may reference some leaf node here from a test parameter.
thread_levels:
  insert_vector:
    standalone:
      mmapv1: [1, 4]
      wiredTiger: [1, 8, 16]
    single-replica:
      mmapv1: [1, 4]
      wiredTiger: [1, 8, 16]
    replica:
      mmapv1: [1, 4]
      wiredTiger: [1, 8, 16]
    shard:
      mmapv1: [1, 32]
      wiredTiger: [1, 32, 64]
  crud-jfalse:
    mmapv1: [1, 4]
    wiredTiger: [1, 16, 32]

pre_task:
  - on_localhost:
      exec: $DSI_PATH/bin/setup-workloads.sh
  - on_workload_client:
      exec: rm -rf workloads*
  - on_workload_client:
      # no order is guaranteed between the two upload files. That is okay here
      upload_files:
        workloads.tar.gz: workloads.tar.gz
      upload_repo_files:
        bin/process_fio_results.py: process_fio_results.py
        bin/process_iperf_results.py: process_iperf_results.py
        bin/fio-test.sh: fio-test.sh
        bin/iperf-test.sh: iperf-test.sh
  - on_workload_client:
      exec: |
        rm -rf workloads
        mkdir workloads
        tar zxvf workloads.tar.gz -C workloads
  - on_all_hosts:
      # Install iperf. It isn't available in yum on AWS instances
      exec: |
        sudo killall iperf3
        rm -rf iperf
        git clone https://github.com/esnet/iperf
        cd iperf/
        git checkout d06415822a
        ./configure
        make
        sudo make install

